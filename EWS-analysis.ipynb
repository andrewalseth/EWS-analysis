{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enduro World Series (EWS) web scraping and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, what is enduro? Basically, it's downhill mountain biking where you have to pedal your way to each stage. Racers are timed on the downhill portion, and then have to pedal their way to the next stage (instead of taking a chair lift, etc.). It looks like:\n",
    "\n",
    "![https://images.app.goo.gl/64AV4ZtHASXin8Ru9](img/muddy_enduro.gif)\n",
    "\n",
    "But also a long day in the saddle. For an example, here's the summary of a race on Strava of a pro enduro racer [Jesse Melamed](https://www.strava.com/activities/7260508291) who took 2nd place (by less than half a second to first!). On the clock, his time was 03:00.67 - wheras the total pedaling time was over three and a half hours! \n",
    "\n",
    "![](img/example_ews.png)\n",
    "\n",
    "Enduro racing at the world stage happens in the Enduro World Series, where the best of the best earn points by winning stages and races. At the end of the season a victor is crowned based on the number of points earned. We're going to take a look at the results in these races and look for trends that identify the types of performances that can crown a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class='strava-embed-placeholder' data-embed-type='activity' data-embed-id='7260508291'></div><script src='https://strava-embeds.com/embed.js'></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the data - web scraping\n",
    "The following cells of this notebook download the results from the EWS from 2018 to current. These dates are chosen, as the PDF files in this range are consistent between events! Like any good data science project, data wrangling takes 80% of the time...\n",
    "\n",
    "First, we begin by downloading results and scraping the files from https://www.enduroworldseries.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import typing_extensions\n",
    "import re\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfWriter, PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to make web scraping more pretty. We're using the `requests` package to to download page information and returning in a cleaned up format using `bs4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"_key\":\"80467108\",\"_id\":\"race_timing/80467108\",\"_rev\":\"_erSSVLu--_\",\"type\":\"event\",\"name\":\"EWS Burke\",\"description\":\"EWS Burke\",\"year\":\"2022\",\"date\":\"2022-08-13\"},{\"_key\":\"80467146\",\"_id\":\"race_timing/80467146\",\"_rev\":\"_erSSVRC--B\",\"type\":\"event\",\"name\":\"EWS Petzen-Jamnica\",\"description\":\"EWS Petzen-Jamnica\",\"year\":\"2022\",\"date\":\"2022-06-18\"},{\"_key\":\"80467177\",\"_id\":\"race_timing/80467177\",\"_rev\":\"_erSSVW---_\",\"type\":\"event\",\"name\":\"EWS Sugarloaf\",\"description\":\"EWS Sugarloaf\",\"year\":\"2022\",\"date\":\"2022-08-20\"},{\"_key\":\"80467197\",\"_id\":\"race_timing/80467197\",\"_rev\":\"_erSSVk---A\",\"type\":\"event\",\"name\":\"EWS Tweed Valley\",\"description\":\"EWS Tweed Valley\",\"year\":\"2022\",\"date\":\"2022-06-04\"},{\"_key\":\"80467222\",\"_id\":\"race_timing/80467222\",\"_rev\":\"_erSSVsG--B\",\"type\":\"event\",\"name\":\"EWS Val Di Fassa\",\"description\":\"EWS Val Di Fassa\",\"year\":\"2022\",\"date\":\"2022-06-25\"},{\"_key\":\"80467247\",\"_id\":\"race_timing/80467247\",\"_rev\":\"_erSSV4e--C\",\"type\":\"event\",\"name\":\"EWS Whistler\",\"description\":\"EWS Whistler\",\"year\":\"2022\",\"date\":\"2022-08-06\"},{\"_key\":\"83394722\",\"_id\":\"race_timing/83394722\",\"_rev\":\"_ezE6Dly---\",\"type\":\"event\",\"name\":\"EWS Crans-Montana\",\"description\":\"EWS Crans-Montana\",\"year\":\"2022\",\"date\":\"2022-09-17\"},{\"_key\":\"84135210\",\"_id\":\"race_timing/84135210\",\"_rev\":\"_e1VGaCq---\",\"type\":\"event\",\"name\":\"EWS Loudenvielle\",\"description\":\"EWS Loudenvielle\",\"year\":\"2022\",\"date\":\"2022-09-24\"},{\"_key\":\"83162658\",\"_id\":\"race_timing/83162658\",\"_rev\":\"_eyZzsge---\",\"type\":\"event\",\"name\":\"EWS-E Crans-Montana\",\"description\":\"EWS-E Crans-Montana\",\"year\":\"2022\",\"date\":\"2022-09-15\"},{\"_key\":\"83165797\",\"_id\":\"race_timing/83165797\",\"_rev\":\"_eyZ05gK---\",\"type\":\"event\",\"name\":\"EWS-E Petzen-Jamnica\",\"description\":\"EWS-E Petzen-Jamnica\",\"year\":\"2022\",\"date\":\"2022-06-16\"},{\"_key\":\"83168307\",\"_id\":\"race_timing/83168307\",\"_rev\":\"_eyZ2XLm---\",\"type\":\"event\",\"name\":\"EWS-E Tweed Valley\",\"description\":\"EWS-E Tweed Valley\",\"year\":\"2022\",\"date\":\"2022-06-02\"},{\"_key\":\"83172372\",\"_id\":\"race_timing/83172372\",\"_rev\":\"_eyZ4Dpi---\",\"type\":\"event\",\"name\":\"EWS-E Valberg\",\"description\":\"EWS-E Valberg\",\"year\":\"2022\",\"date\":\"2022-07-02\"},{\"_key\":\"84586215\",\"_id\":\"race_timing/84586215\",\"_rev\":\"_e2ln_Za---\",\"type\":\"event\",\"name\":\"EWS-E Finale Ligure\",\"description\":\"EWS-E Finale Ligure\",\"year\":\"2022\",\"date\":\"2022-09-29\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://a23ea854a37f.arangodb.cloud:8529/_db/EWSDB/api_production//race_names/2022\"\n",
    "\n",
    "payload = \"\"\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Authorization\": \"Basic QVBJX0VXUzpJRG9BUElUaGluZ3NGb3JQZW9wbGUuITI=\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Origin\": \"https://www.enduroworldseries.com\",\n",
    "    \"Referer\": \"https://www.enduroworldseries.com/\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"macOS\",\n",
    "    \"sec-gpc\": \"1\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, data=payload, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url):\n",
    "\theaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\n",
    "\ttry:\n",
    "\t\treq.raise_for_status()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Downloading failed: {e}')\n",
    "\t\n",
    "\treturn bs4.BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download the pages for each of the race years for standard EWS race results and EMTB race results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emtb_result = \"https://www.enduroworldseries.com/races/6/\" \n",
    "ews_result = \"https://www.enduroworldseries.com/races/1/\"\n",
    "\n",
    "emtb_years = [2020, 2021, 2022]\n",
    "ews_years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "list_of_emtb_soup = [download_page(emtb_result + str(year) + '/') for year in emtb_years]\n",
    "\n",
    "list_of_ews_soup = [download_page(ews_result + str(year) + '/') for year in ews_years]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto downlading the actual url's which contain the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_links(results_page, result_url, year):\n",
    "\tlinks = results_page.find_all('a', href=True)\n",
    "\tlinks = [link.get('href') for link in links]\n",
    "\n",
    "\trace_list_URL = [result_url[:-9]+str(link_text) for link_text in links if 'results' in link_text]\n",
    "\n",
    "\treturn [(url, url.split('/')[-4]) for url in race_list_URL if str(year) in url] # returns the tuple of the URL and the name of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_emtb_results = [get_result_links(list_of_emtb_soup[i], emtb_result, emtb_years[i]) for i in range(len(emtb_years))]\n",
    "links_to_ews_results = [get_result_links(list_of_ews_soup[i], ews_result, ews_years[i]) for i in range(len(ews_years))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(soup):\n",
    "\tHTML_source = soup.find_all('a', href=True)\n",
    "\tsource_links = [(link.get('href'), link) for link in HTML_source]\n",
    "\n",
    "\treturn [(pdf, pdf_link.get_text()) for pdf, pdf_link in source_links if '.pdf' in pdf]\n",
    "\n",
    "def download_pdf(url, folder_path, filename):\n",
    "\treq = requests.get(url)\n",
    "\treq.raise_for_status()\n",
    "\n",
    "\twith open(folder_path + filename, 'wb') as f:\n",
    "\t\tf.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_downloads(result_links, years, folder_path):\n",
    "\tfor i in range(len(years)):\n",
    "\n",
    "\t\tyear = years[i]\n",
    "\n",
    "\t\tfor race_link, race_name in result_links[i]:\n",
    "\n",
    "\t\t\trace_page = download_page(race_link)\n",
    "\t\t\tresult_pdf = find_pdf_links(race_page)\n",
    "\n",
    "\t\t\tfor pdf_link, pdf_text in result_pdf:\n",
    "\n",
    "\t\t\t\tfilename = str(year)+ '_' + race_name + '_' + pdf_text.replace(' ', '_') + '.pdf'\n",
    "\t\t\t\tdownload_pdf(pdf_link, folder_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells download all the pdfs for the desired years based off of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_emtb_results, emtb_years, 'emtb_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_ews_results, ews_years, 'ews_results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the PDF and placing into dataframes\n",
    "Now that we have read in the various PDF, we need to pull the data out into a useable format. PDF's are tricky beasts, so we're going to rely on the `PyPDF2` package to take these data in. Unfortunately, these PDF are not set up as tables (otherwise this would be a trivial import using the `camelot` package), so we need to use a bunch of regular expressions to extract the desired data. Then, we place the data in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various regular expressions to extract data from text of results PDF\n",
    "stage_numbers_regex = re.compile(r'Stage \\d')\t\t\t\t\t# recognizes stage numbers in headers of PDF\n",
    "position_plate_name_regex = re.compile(r'\\d+\\s\\d+\\s[^a-z]+[a-z]+\\s')\t\t# finds rider position and name from individuals\n",
    "dnf_dns_plate_name_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+\\s[^a-z]+[a-z]+\\s')\t# finds DNF/DNS rider information \n",
    "stage_position_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d \\d+') \t\t\t# recognizes the a stage with its position\n",
    "stage_time_regex = re.compile(r'(\\d:\\d\\d:\\d\\d\\.\\d\\d)') \t\t\t\t# recognizes each stage time (assumes all stages  <10 hours)\n",
    "gap_regex = re.compile(r'\\+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t\t\t# determines gap from overall leader \n",
    "penalty_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d\\s+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t# penalty values occur before overall stage results\n",
    "rider_id_regex = re.compile(r'\\w{3}\\.[\\d\\w\\s]+\\.[\\d\\w]+') \t\t\t# gets rider ID from results\n",
    "lastname_regex = re.compile(r'\\s[^a-z0-9]+')\t\t\t\t\t# recognizes lastname - located between plate and firstname, no lowercase or numbers\n",
    "firstname_regex = re.compile(r'([A-Z][a-z]+\\s)+')\t\t\t\t# recognizes firstname - first capital letter then lowercase TODO make sure this matches correctly \n",
    "position_plate_regex = re.compile(r'\\d+\\s\\d+')\t\t\t\t\t# recognizes the position and plate\n",
    "dnf_dns_plate_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+')\t\t\t\t# recognizes DNF/DNS/DSQ along with plate\n",
    "penalties_details_regex = re.compile(r'Penalties details')\n",
    "penalty_line_regex = re.compile(r'DNF: did not finish   ·   DNS: did not start   ·   DSQ: disqualified')\n",
    "\n",
    "#TODO need better regex for firstname/lastname - see G.T. CLYNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ugly, but this is the function which reads the results PDF files and converts to csv. We convert to csv for easy storage and loading into Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ews_pdf_to_csv(pdf_location, csv_location='csv_output/'):\n",
    "\n",
    "\t# read in PDF and convert each page to list of strings\n",
    "\treader = PdfReader(pdf_location)\n",
    "\tpages = [page.extract_text().split('\\n') for page in reader.pages] # newline separates lines on all pages\n",
    "\n",
    "\tpdf_header = pages[0][:5]\n",
    "\tcolumns = pdf_header[0] + pdf_header[1] # first two lines are the column names for the file\n",
    "\n",
    "\t# race information\n",
    "\tnum_stages = len(stage_numbers_regex.findall(columns)) # store the total number of stages based upon header\n",
    "\trace_date = pdf_header[3]\n",
    "\trace_location = pdf_header[2]\n",
    "\trace_type = 'standard'\n",
    "\n",
    "\trace_info = [race_date, race_location, race_type]\n",
    "\n",
    "\theader_race_info = ['date', 'race_location', 'race_type']\n",
    "\theader_rider_info = ['rider_category','rider_plate', 'rider_lastname', 'rider_firstname', 'rider_id', 'rider_final_position',\n",
    "\t\t'rider_penalties' , 'rider_final_time' , 'gap_from_first']\n",
    "\theader_rider_stage_results = ['stage_'+str(i)+'_time' for i in range(1,num_stages+1)] + ['stage_'+str(i)+'_pos' for i in range(1,num_stages+1)]\n",
    "\t# df_list = [['rider_num', 'rider_name', 'rider_id', 'rider_final_position' + 'rider_final_time'] + ['stage_'+str(i)+'time']]\n",
    "\n",
    "\trace_header_info = [header_race_info + header_rider_info + header_rider_stage_results]\n",
    "\n",
    "\tall_results = race_header_info\n",
    "\n",
    "\n",
    "\tfor page in pages:\n",
    "\n",
    "\t\tis_results_page = pdf_header[0] == page[0] # checks if the first line of the page matches the header\n",
    "\t\t\n",
    "\t\t# try:\n",
    "\t\t# \tis_penalty_page = penalties_details_regex.search(page[4])\n",
    "\t\t# except IndexError:\n",
    "\t\tis_penalty_page = False\n",
    "\n",
    "\t\tfor line in page:\n",
    "\t\t\tif penalty_line_regex.search(line):\n",
    "\t\t\t\tis_penalty_page = True\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\t\t# if page == pages[-1]:\n",
    "\t\t# \tfor j in range(7):\n",
    "\t\t# \t\tprint(f'line {j} = {page[j]}')\n",
    "\n",
    "\t\tif is_penalty_page:\n",
    "\t\t\t# print(is_penalty_page)\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif is_results_page:\n",
    "\t\t\ti = 5 # start after header\n",
    "\t\t\trider_catagory = ''\n",
    "\n",
    "\t\t\t# iterate over all lines except final (which contains metadata)\n",
    "\t\t\twhile i < len(page) - 1:\n",
    "\n",
    "\t\t\t\tppnr = position_plate_name_regex.search(page[i])\n",
    "\t\t\t\tddr = dnf_dns_plate_name_regex.search(page[i])\n",
    "\n",
    "\t\t\t\tif ppnr or ddr: # check if line contains rider information \n",
    "\t\t\t\t\tresult = copy.deepcopy(race_info)\n",
    "\n",
    "\t\t\t\t\tline1 = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tline2 = page[i]\n",
    "\n",
    "\t\t\t\t\tfix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "\t\t\t\t\tfix = fix.replace('+ ', '+') # removes space before gap time\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tinfo = ppnr.group()\n",
    "\t\t\t\t\t\tppr = position_plate_regex.search(info)\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tinfo = ddr.group()\n",
    "\t\t\t\t\t\tppr = dnf_dns_plate_regex.search(info)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tposition, plate = ppr.group().split(' ')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tlastname = lastname_regex.search(info).group()\n",
    "\t\t\t\t\tlastname = lastname[1:-2]\n",
    "\t\t\t\t\tfirstname = firstname_regex.search(info).group()\n",
    "\n",
    "\t\t\t\t\tspr = stage_position_regex.findall(fix)\n",
    "\t\t\t\t\tspr = [s.split(' ') for s in spr]\t\n",
    "\n",
    "\t\t\t\t\trir = rider_id_regex.search(fix)\n",
    "\t\t\t\t\tpr = penalty_regex.search(line1)\n",
    "\t\t\t\t\tgr = gap_regex.search(fix)\n",
    "\t\t\t\t\tst = stage_time_regex.findall(line1)\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\trider_num = None\n",
    "\t\t\t\t\tif rir:\n",
    "\t\t\t\t\t\trider_num = rir.group()\n",
    "\n",
    "\n",
    "\t\t\t\t\tpenalty_time = None\n",
    "\t\t\t\t\tif pr:\n",
    "\t\t\t\t\t\tpenalty_time = pr.group().split(' ')[0]\n",
    "\n",
    "\t\t\t\t\tresult += [rider_category, plate, lastname, firstname, rider_num, position, penalty_time]\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tif gr:\n",
    "\t\t\t\t\t\t\tfinal_time = st[-2]\n",
    "\t\t\t\t\t\t\tgap \t   = st[-1]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif not len(st): # for instances where no st regex is found\n",
    "\t\t\t\t\t\t\t\tprint(f'string: {page[i]}\\nline: {i}') # \n",
    "\t\t\t\t\t\t\tfinal_time = st[-1]\n",
    "\t\t\t\t\t\t\tgap = '0:00:00.00'\n",
    "\n",
    "\t\t\t\t\t\tresult += [final_time, gap]\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr]\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult += [None, None] # no gap or final time for DNF/DNS/DSQ\n",
    "\t\t\t\t\t\tstage_diff = num_stages - len(spr) # calculate how many stages were not completed\n",
    "\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\n",
    "\t\t\t\t\tall_results.append(result)\n",
    "\t\t\t\t\ti += 1\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telse:\t# otherwise, this is category information for the following riders\n",
    "\t\t\t\t\trider_category = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\n",
    "\tpdf_filename = os.path.split(pdf_location)[1][:-4]\n",
    "\n",
    "\twith open(csv_location + pdf_filename + '.csv', 'w', newline='') as cw:\n",
    "\t\twriter = csv.writer(cw)\n",
    "\t\tfor row in all_results:\n",
    "\t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the script and saving to .csv\n",
    "\n",
    "Now that we've got something to convert these standardized pdf's to csv, we run them through the script and create new csv for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('ews_results'):\n",
    "\tif file.endswith('.pdf') and '_Results' in file:\n",
    "\t\ttry:\n",
    "\t\t\tews_pdf_to_csv('ews_results/'+file)\n",
    "\t\texcept IndexError as e:\n",
    "\t\t\tprint(f'IndexError: {e}\\nFile {file}')\n",
    "\t\t\tprint(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e5ec9c443fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_directory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_column_name\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "csv_directory = ['csv_output/'+ file for file in os.listdir('csv_output')]\n",
    "df_list = []\n",
    "\n",
    "df = pd.read_csv(csv_directory[0], header=0)\n",
    "\n",
    "for file in csv_directory:\n",
    "\tdf_list.append(pd.read_csv(file,index_col=False))\n",
    "\n",
    "\n",
    "# df = pd.concat(df_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'csv_output/2021_MichelinEWSLoudenvielle1_Results_EWS_1.csv'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_directory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-286e2bcbdf72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ews_pdf_to_csv(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('raw_pdf', 'test3.pdf')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv_output/test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ed342939cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpdf_header\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "pages[-1][0] == pdf_header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results[0]) == len(all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March 24 - 25, 2018',\n",
       " 'Lo Barnechea, Chile',\n",
       " 'standard',\n",
       " 'MEN',\n",
       " '1',\n",
       " 'HILL',\n",
       " 'Sam',\n",
       " 'AUS.1985.21775',\n",
       " '1',\n",
       " None,\n",
       " '0:55:02.18',\n",
       " '0:00:00.00',\n",
       " '0:05:12.25',\n",
       " '0:18:07.96',\n",
       " '0:05:26.70',\n",
       " '0:09:55.25',\n",
       " '0:04:07.43',\n",
       " '0:12:12.59',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2 = page2text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = '4 13 HILL Sam 0:03:28.58 22 0:05:44.14 28 0:04:55.69 190:06:44.10 1 0:37:40.33 +0:00:08.61'\n",
    "line2 = '  Chain Reaction Cycles Mavic AUS.HILS.1985 0:07:19.73 7 0:04:17.81 20:05:10.28 1'\n",
    "\n",
    "line1 = '147 122 DA SILVA Goncalo 0:04:11.38 125 0:09:38.38 157 0:07:50.37 154 0:11:31.81 153 0:01:00.00 1:10:34.72 +0:33:03.00'\n",
    "line2 = ' POR.DA G.1987 0:19:28.41 147 0:08:13.09 148 0:08:41.28 148'\n",
    "\n",
    "fix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "fix = fix.replace('+ ', '+') # removes space before gap time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'147 122 DA SILVA Goncalo  0:04:11.38 125  0:09:38.38 157  0:07:50.37 154  0:11:31.81 153  0:01:00.00  1:10:34.72 +0:33:03.00 POR.DA G.1987  0:19:28.41 147  0:08:13.09 148  0:08:41.28 148'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnr = position_plate_name_regex.search(fix)\n",
    "spr = stage_position_regex.findall(fix)\n",
    "rir = rider_id_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = penalty_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:01:00.00  1:10:34.72'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'147 122 DA SILVA Goncalo '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppnr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = [s.split(' ') for s in spr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0:04:11.38', '125'],\n",
       " ['0:09:38.38', '157'],\n",
       " ['0:07:50.37', '154'],\n",
       " ['0:11:31.81', '153'],\n",
       " ['0:19:28.41', '147'],\n",
       " ['0:08:13.09', '148'],\n",
       " ['0:08:41.28', '148']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POR.DA G.1987'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rir.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95fc6b46201b03e388ee93255aacead15bb4c5a805a1325bd67fb6d36cada86c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
