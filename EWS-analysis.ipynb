{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enduro World Series (EWS) web scraping and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, what is enduro? Basically, it's downhill mountain biking where you have to pedal your way to each stage. Racers are timed on the downhill portion, and then have to pedal their way to the next stage (instead of taking a chair lift, etc.). It looks like:\n",
    "\n",
    "![https://images.app.goo.gl/64AV4ZtHASXin8Ru9](img/muddy_enduro.gif)\n",
    "\n",
    "But also a long day in the saddle. For an example, here's the summary of a race on Strava of a pro enduro racer [Jesse Melamed](https://www.strava.com/activities/7260508291) who took 2nd place (by less than half a second to first!). On the clock, his time was 03:00.67 - wheras the total pedaling time was over three and a half hours!\n",
    "\n",
    "![](img/example_ews.png)\n",
    "\n",
    "Enduro racing at the world stage happens in the Enduro World Series, where the best of the best earn points by winning stages and races. At the end of the season a victor is crowned based on the number of points earned. We're going to take a look at the results in these races and look for trends that identify the types of performances that can crown a winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the data - web scraping\n",
    "The following cells of this notebook download the results from the EWS for 2022. We only use the ! Like any good data science project, data wrangling takes 80% of the time...\n",
    "\n",
    "First, we begin by downloading results and scraping the files from https://www.enduroworldseries.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import typing_extensions\n",
    "import re\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfWriter, PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to make web scraping more pretty. We're using the `requests` package to make requests to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo find out the file structure for races - it seems that each result is sorted by class in the form //race_results/class/class#\n",
    "#todo determine the classes and class numbers present\n",
    "\n",
    "import requests\n",
    "\n",
    "base_url = \"https://a23ea854a37f.arangodb.cloud:8529/_db/EWSDB/api_production//\"\n",
    "\n",
    "payload = \"\"\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Authorization\": \"Basic QVBJX0VXUzpJRG9BUElUaGluZ3NGb3JQZW9wbGUuITI=\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Origin\": \"https://www.enduroworldseries.com\",\n",
    "    \"Referer\": \"https://www.enduroworldseries.com/\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"macOS\",\n",
    "    \"sec-gpc\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_json_dict(url, payload=payload, headers=headers, save=False, folder=\"\", filename=\"\", year=\"2022\"):\n",
    "\tresults = requests.request(\"GET\", url, data=payload, headers=headers)\n",
    "\t\n",
    "\tif save:\n",
    "\t\twith open(year+folder+filename+\".json\", 'w+') as f:\n",
    "\t\t\tjson.dump(results, f)\n",
    "\n",
    "\treturn json.loads(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_json_string(url, payload=payload, headers=headers, save=False, folder=\"\", filename=\"\", year=\"2022\"):\n",
    "\tresults = requests.request(\"GET\", url, data=payload, headers=headers)\n",
    "\t\n",
    "\tif save:\n",
    "\t\twith open(year+folder+filename+\".json\", 'w+') as f:\n",
    "\t\t\tjson.dump(results, f)\n",
    "\n",
    "\treturn results.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_races_2022 = \"race_names/2022\"\n",
    "race_information = url_to_json_dict(base_url+url_races_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_names_2022 = [race['description'] for race in race_information]\n",
    "\n",
    "race_url_strings_2022 = {race:race.replace(' ', '%20') for race in race_names_2022}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_classes_2022 = {race:url_to_json_dict(base_url+\"race_classes/2022/\"+race_string) for race, race_string in race_url_strings_2022.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual rider query: race_results/rider/[rider class]/[rider #]\n",
    "\n",
    "rider_result_test = url_to_json_dict(base_url+\"race_results/rider/80467121/22930\")\n",
    "\n",
    "rider_result_test = rider_result_test[0]\n",
    "\n",
    "results_format = ['time', 'stage_result', 'cumulative_result', 'cumulative_behind', 'overall_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rider_result_test[0]['stage'][:7].lower().replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom unpacking of data - convert data to columns\n",
    "def unpack_stage_results(rider_results, rider_id, results_format=results_format, save=False):\n",
    "\ti = 1\n",
    "\toffset = 0\n",
    "\theader = []\n",
    "\tresults = []\n",
    "\twhile i < len(rider_results) + 1:\n",
    "\t\tstage_data = rider_results[i-1]['stage'] # trims results from format of 'Stage 1PRO' to 'Stage 1' in case of pro/queen stage\n",
    "\t\tif len(stage_data) > 7:\n",
    "\t\t\tstage_data = stage_data[:7]\n",
    "\t\tstage_info = stage_data.lower().replace(\" \", \"_\") # modifies results from format of 'Stage 1' to 'stage_1'\n",
    "\t\tstage_info = stage_info + \"_\"\n",
    "\t\tfor result in results_format:\n",
    "\t\t\theader.append(stage_info + result)\n",
    "\t\t\tresults.append(rider_results[i-1][result])\n",
    "\n",
    "\t\ti +=1\n",
    "\n",
    "\treturn ['rider_id']+header, [rider_id]+results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_class = \"80467121\"\n",
    "# TODO adjust for riders not having results in all categories\n",
    "\n",
    "#for race in race_classes_2022['EWS Burke']:\n",
    "\n",
    "results_dict = dict()\n",
    "\n",
    "for race_name in race_classes_2022.keys():\n",
    "\n",
    "# race_name = 'EWS Burke'\n",
    "\n",
    "\tfor race_class in race_classes_2022[race_name]:\n",
    "\t#for race_class in [{'name': 'EWS80 | MEN', '_key': '80470280'},{'name': 'MEN', '_key': '80467139'}]:\n",
    "\n",
    "\t\t# race information for a specific race class\n",
    "\t\trace_class_key = race_class['_key']\n",
    "\t\trace_class_desc = race_class['name']\n",
    "\n",
    "\t\t# download race results for a race class\n",
    "\t\trider_class_results = url_to_json_dict(base_url+\"race_results/class/\"+race_class_key+\"/1000/0\")\n",
    "\t\trider_class_df = pd.json_normalize(rider_class_results, 'results')\n",
    "\n",
    "\t\t# the ID for riders in each class (used to download specific results)\n",
    "\t\trider_id_list = rider_class_df['rider_id']\n",
    "\n",
    "\t\tstage_class_results = []\n",
    "\t\t\n",
    "\n",
    "\t\tfor rider_id in rider_id_list:\n",
    "\t\t\tindividual_results = url_to_json_dict(f\"{base_url}race_results/rider/{race_class_key}/{rider_id}\")\n",
    "\t\t\tindividual_results = individual_results[0]\n",
    "\n",
    "\t\t\tif len(stage_class_results) == 0:\n",
    "\t\t\t\theader, results = unpack_stage_results(individual_results, rider_id)\n",
    "\t\t\t\tstage_class_results = [header, results]\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t_, results = unpack_stage_results(individual_results, rider_id)\n",
    "\t\t\t\tstage_class_results.append(results)\n",
    "\n",
    "\t\tstage_class_df = pd.DataFrame(stage_class_results[1:], columns=stage_class_results[0])\n",
    "\n",
    "\t\tfull_rider_results = pd.merge(rider_class_df, stage_class_df, how='left', on='rider_id')\n",
    "\n",
    "\t\t# remove the '_key' column\n",
    "\t\tfull_rider_results.drop('_key', inplace=True, axis=1)\n",
    "\n",
    "\t\t# add in the race class\n",
    "\t\tfull_rider_results.insert(0, 'race_class', value=race_class_desc)\n",
    "\n",
    "\t\t# add in the race name at the beginning of the dataframe\n",
    "\t\tfull_rider_results.insert(0, 'race_name', value=race_name)\n",
    "\n",
    "\t\tresults_dict.update({race_name + \"_\" + race_class_desc : full_rider_results})\n",
    "\n",
    "\t\tfull_rider_results.to_csv(race_name + \"_2022_\" + race_class_desc + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EWS_2022_results = pd.concat([results for race_class, results in results_dict.items() if 'EWS ' in race_class])\n",
    "EWS_2022_results.to_csv('EWS_2022_results_by_race.csv',index=False)\n",
    "\n",
    "# EWSE_2022_results = pd.concat([results for race_class, results in results_dict.items() if 'EWS-E' in race_class])\n",
    "# EWSE_2022_results.to_csv('EWS-E_2022_results_by_race.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stage_results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df = pd.DataFrame(stage_results[1:], columns=stage_results[0])\n",
    "stage_df.rider_id = stage_df.rider_id.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riders.rider_id = riders.rider_id.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data together\n",
    "\n",
    "pd.merge(riders, stage_df, how='left', on='rider_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rider_results_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(rider_result_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url):\n",
    "\theaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\n",
    "\ttry:\n",
    "\t\treq.raise_for_status()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Downloading failed: {e}')\n",
    "\t\n",
    "\treturn bs4.BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download the pages for each of the race years for standard EWS race results and EMTB race results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emtb_result = \"https://www.enduroworldseries.com/races/6/\" \n",
    "ews_result = \"https://www.enduroworldseries.com/races/1/\"\n",
    "\n",
    "emtb_years = [2020, 2021, 2022]\n",
    "ews_years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "list_of_emtb_soup = [download_page(emtb_result + str(year) + '/') for year in emtb_years]\n",
    "\n",
    "list_of_ews_soup = [download_page(ews_result + str(year) + '/') for year in ews_years]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto downlading the actual url's which contain the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_links(results_page, result_url, year):\n",
    "\tlinks = results_page.find_all('a', href=True)\n",
    "\tlinks = [link.get('href') for link in links]\n",
    "\n",
    "\trace_list_URL = [result_url[:-9]+str(link_text) for link_text in links if 'results' in link_text]\n",
    "\n",
    "\treturn [(url, url.split('/')[-4]) for url in race_list_URL if str(year) in url] # returns the tuple of the URL and the name of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_emtb_results = [get_result_links(list_of_emtb_soup[i], emtb_result, emtb_years[i]) for i in range(len(emtb_years))]\n",
    "links_to_ews_results = [get_result_links(list_of_ews_soup[i], ews_result, ews_years[i]) for i in range(len(ews_years))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(soup):\n",
    "\tHTML_source = soup.find_all('a', href=True)\n",
    "\tsource_links = [(link.get('href'), link) for link in HTML_source]\n",
    "\n",
    "\treturn [(pdf, pdf_link.get_text()) for pdf, pdf_link in source_links if '.pdf' in pdf]\n",
    "\n",
    "def download_pdf(url, folder_path, filename):\n",
    "\treq = requests.get(url)\n",
    "\treq.raise_for_status()\n",
    "\n",
    "\twith open(folder_path + filename, 'wb') as f:\n",
    "\t\tf.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_downloads(result_links, years, folder_path):\n",
    "\tfor i in range(len(years)):\n",
    "\n",
    "\t\tyear = years[i]\n",
    "\n",
    "\t\tfor race_link, race_name in result_links[i]:\n",
    "\n",
    "\t\t\trace_page = download_page(race_link)\n",
    "\t\t\tresult_pdf = find_pdf_links(race_page)\n",
    "\n",
    "\t\t\tfor pdf_link, pdf_text in result_pdf:\n",
    "\n",
    "\t\t\t\tfilename = str(year)+ '_' + race_name + '_' + pdf_text.replace(' ', '_') + '.pdf'\n",
    "\t\t\t\tdownload_pdf(pdf_link, folder_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells download all the pdfs for the desired years based off of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_emtb_results, emtb_years, 'emtb_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_ews_results, ews_years, 'ews_results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the PDF and placing into dataframes\n",
    "Now that we have read in the various PDF, we need to pull the data out into a useable format. PDF's are tricky beasts, so we're going to rely on the `PyPDF2` package to take these data in. Unfortunately, these PDF are not set up as tables (otherwise this would be a trivial import using the `camelot` package), so we need to use a bunch of regular expressions to extract the desired data. Then, we place the data in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various regular expressions to extract data from text of results PDF\n",
    "stage_numbers_regex = re.compile(r'Stage \\d')\t\t\t\t\t# recognizes stage numbers in headers of PDF\n",
    "position_plate_name_regex = re.compile(r'\\d+\\s\\d+\\s[^a-z]+[a-z]+\\s')\t\t# finds rider position and name from individuals\n",
    "dnf_dns_plate_name_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+\\s[^a-z]+[a-z]+\\s')\t# finds DNF/DNS rider information \n",
    "stage_position_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d \\d+') \t\t\t# recognizes the a stage with its position\n",
    "stage_time_regex = re.compile(r'(\\d:\\d\\d:\\d\\d\\.\\d\\d)') \t\t\t\t# recognizes each stage time (assumes all stages  <10 hours)\n",
    "gap_regex = re.compile(r'\\+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t\t\t# determines gap from overall leader \n",
    "penalty_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d\\s+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t# penalty values occur before overall stage results\n",
    "rider_id_regex = re.compile(r'\\w{3}\\.[\\d\\w\\s]+\\.[\\d\\w]+') \t\t\t# gets rider ID from results\n",
    "lastname_regex = re.compile(r'\\s[^a-z0-9]+')\t\t\t\t\t# recognizes lastname - located between plate and firstname, no lowercase or numbers\n",
    "firstname_regex = re.compile(r'([A-Z][a-z]+\\s)+')\t\t\t\t# recognizes firstname - first capital letter then lowercase TODO make sure this matches correctly \n",
    "position_plate_regex = re.compile(r'\\d+\\s\\d+')\t\t\t\t\t# recognizes the position and plate\n",
    "dnf_dns_plate_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+')\t\t\t\t# recognizes DNF/DNS/DSQ along with plate\n",
    "penalties_details_regex = re.compile(r'Penalties details')\n",
    "penalty_line_regex = re.compile(r'DNF: did not finish   ·   DNS: did not start   ·   DSQ: disqualified')\n",
    "\n",
    "#TODO need better regex for firstname/lastname - see G.T. CLYNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ugly, but this is the function which reads the results PDF files and converts to csv. We convert to csv for easy storage and loading into Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ews_pdf_to_csv(pdf_location, csv_location='csv_output/'):\n",
    "\n",
    "\t# read in PDF and convert each page to list of strings\n",
    "\treader = PdfReader(pdf_location)\n",
    "\tpages = [page.extract_text().split('\\n') for page in reader.pages] # newline separates lines on all pages\n",
    "\n",
    "\tpdf_header = pages[0][:5]\n",
    "\tcolumns = pdf_header[0] + pdf_header[1] # first two lines are the column names for the file\n",
    "\n",
    "\t# race information\n",
    "\tnum_stages = len(stage_numbers_regex.findall(columns)) # store the total number of stages based upon header\n",
    "\trace_date = pdf_header[3]\n",
    "\trace_location = pdf_header[2]\n",
    "\trace_type = 'standard'\n",
    "\n",
    "\trace_info = [race_date, race_location, race_type]\n",
    "\n",
    "\theader_race_info = ['date', 'race_location', 'race_type']\n",
    "\theader_rider_info = ['rider_category','rider_plate', 'rider_lastname', 'rider_firstname', 'rider_id', 'rider_final_position',\n",
    "\t\t'rider_penalties' , 'rider_final_time' , 'gap_from_first']\n",
    "\theader_rider_stage_results = ['stage_'+str(i)+'_time' for i in range(1,num_stages+1)] + ['stage_'+str(i)+'_pos' for i in range(1,num_stages+1)]\n",
    "\t# df_list = [['rider_num', 'rider_name', 'rider_id', 'rider_final_position' + 'rider_final_time'] + ['stage_'+str(i)+'time']]\n",
    "\n",
    "\trace_header_info = [header_race_info + header_rider_info + header_rider_stage_results]\n",
    "\n",
    "\tall_results = race_header_info\n",
    "\n",
    "\n",
    "\tfor page in pages:\n",
    "\n",
    "\t\tis_results_page = pdf_header[0] == page[0] # checks if the first line of the page matches the header\n",
    "\t\t\n",
    "\t\t# try:\n",
    "\t\t# \tis_penalty_page = penalties_details_regex.search(page[4])\n",
    "\t\t# except IndexError:\n",
    "\t\tis_penalty_page = False\n",
    "\n",
    "\t\tfor line in page:\n",
    "\t\t\tif penalty_line_regex.search(line):\n",
    "\t\t\t\tis_penalty_page = True\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\t\t# if page == pages[-1]:\n",
    "\t\t# \tfor j in range(7):\n",
    "\t\t# \t\tprint(f'line {j} = {page[j]}')\n",
    "\n",
    "\t\tif is_penalty_page:\n",
    "\t\t\t# print(is_penalty_page)\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif is_results_page:\n",
    "\t\t\ti = 5 # start after header\n",
    "\t\t\trider_catagory = ''\n",
    "\n",
    "\t\t\t# iterate over all lines except final (which contains metadata)\n",
    "\t\t\twhile i < len(page) - 1:\n",
    "\n",
    "\t\t\t\tppnr = position_plate_name_regex.search(page[i])\n",
    "\t\t\t\tddr = dnf_dns_plate_name_regex.search(page[i])\n",
    "\n",
    "\t\t\t\tif ppnr or ddr: # check if line contains rider information \n",
    "\t\t\t\t\tresult = copy.deepcopy(race_info)\n",
    "\n",
    "\t\t\t\t\tline1 = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tline2 = page[i]\n",
    "\n",
    "\t\t\t\t\tfix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "\t\t\t\t\tfix = fix.replace('+ ', '+') # removes space before gap time\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tinfo = ppnr.group()\n",
    "\t\t\t\t\t\tppr = position_plate_regex.search(info)\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tinfo = ddr.group()\n",
    "\t\t\t\t\t\tppr = dnf_dns_plate_regex.search(info)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tposition, plate = ppr.group().split(' ')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tlastname = lastname_regex.search(info).group()\n",
    "\t\t\t\t\tlastname = lastname[1:-2]\n",
    "\t\t\t\t\tfirstname = firstname_regex.search(info).group()\n",
    "\n",
    "\t\t\t\t\tspr = stage_position_regex.findall(fix)\n",
    "\t\t\t\t\tspr = [s.split(' ') for s in spr]\t\n",
    "\n",
    "\t\t\t\t\trir = rider_id_regex.search(fix)\n",
    "\t\t\t\t\tpr = penalty_regex.search(line1)\n",
    "\t\t\t\t\tgr = gap_regex.search(fix)\n",
    "\t\t\t\t\tst = stage_time_regex.findall(line1)\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\trider_num = None\n",
    "\t\t\t\t\tif rir:\n",
    "\t\t\t\t\t\trider_num = rir.group()\n",
    "\n",
    "\n",
    "\t\t\t\t\tpenalty_time = None\n",
    "\t\t\t\t\tif pr:\n",
    "\t\t\t\t\t\tpenalty_time = pr.group().split(' ')[0]\n",
    "\n",
    "\t\t\t\t\tresult += [rider_category, plate, lastname, firstname, rider_num, position, penalty_time]\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tif gr:\n",
    "\t\t\t\t\t\t\tfinal_time = st[-2]\n",
    "\t\t\t\t\t\t\tgap \t   = st[-1]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif not len(st): # for instances where no st regex is found\n",
    "\t\t\t\t\t\t\t\tprint(f'string: {page[i]}\\nline: {i}') # \n",
    "\t\t\t\t\t\t\tfinal_time = st[-1]\n",
    "\t\t\t\t\t\t\tgap = '0:00:00.00'\n",
    "\n",
    "\t\t\t\t\t\tresult += [final_time, gap]\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr]\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult += [None, None] # no gap or final time for DNF/DNS/DSQ\n",
    "\t\t\t\t\t\tstage_diff = num_stages - len(spr) # calculate how many stages were not completed\n",
    "\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\n",
    "\t\t\t\t\tall_results.append(result)\n",
    "\t\t\t\t\ti += 1\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telse:\t# otherwise, this is category information for the following riders\n",
    "\t\t\t\t\trider_category = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\n",
    "\tpdf_filename = os.path.split(pdf_location)[1][:-4]\n",
    "\n",
    "\twith open(csv_location + pdf_filename + '.csv', 'w', newline='') as cw:\n",
    "\t\twriter = csv.writer(cw)\n",
    "\t\tfor row in all_results:\n",
    "\t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the script and saving to .csv\n",
    "\n",
    "Now that we've got something to convert these standardized pdf's to csv, we run them through the script and create new csv for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('ews_results'):\n",
    "\tif file.endswith('.pdf') and '_Results' in file:\n",
    "\t\ttry:\n",
    "\t\t\tews_pdf_to_csv('ews_results/'+file)\n",
    "\t\texcept IndexError as e:\n",
    "\t\t\tprint(f'IndexError: {e}\\nFile {file}')\n",
    "\t\t\tprint(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "csv_directory = ['csv_output/'+ file for file in os.listdir('csv_output')]\n",
    "df_list = []\n",
    "\n",
    "df = pd.read_csv(csv_directory[0], header=0)\n",
    "\n",
    "for file in csv_directory:\n",
    "\tdf_list.append(pd.read_csv(file,index_col=False))\n",
    "\n",
    "\n",
    "# df = pd.concat(df_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ews_pdf_to_csv(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.split(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv_output/test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[-1][0] == pdf_header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_results[0]) == len(all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2 = page2text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = '4 13 HILL Sam 0:03:28.58 22 0:05:44.14 28 0:04:55.69 190:06:44.10 1 0:37:40.33 +0:00:08.61'\n",
    "line2 = '  Chain Reaction Cycles Mavic AUS.HILS.1985 0:07:19.73 7 0:04:17.81 20:05:10.28 1'\n",
    "\n",
    "line1 = '147 122 DA SILVA Goncalo 0:04:11.38 125 0:09:38.38 157 0:07:50.37 154 0:11:31.81 153 0:01:00.00 1:10:34.72 +0:33:03.00'\n",
    "line2 = ' POR.DA G.1987 0:19:28.41 147 0:08:13.09 148 0:08:41.28 148'\n",
    "\n",
    "fix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "fix = fix.replace('+ ', '+') # removes space before gap time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnr = position_plate_name_regex.search(fix)\n",
    "spr = stage_position_regex.findall(fix)\n",
    "rir = rider_id_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = penalty_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = [s.split(' ') for s in spr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rir.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95fc6b46201b03e388ee93255aacead15bb4c5a805a1325bd67fb6d36cada86c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
