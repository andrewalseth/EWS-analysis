{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enduro World Series (EWS) web scraping and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, what is enduro? Basically, it's downhill mountain biking where you have to pedal your way to each stage. Racers are timed on the downhill portion, and then have to pedal their way to the next stage (instead of taking a chair lift, etc.). It looks like:\n",
    "\n",
    "![https://images.app.goo.gl/64AV4ZtHASXin8Ru9](img/muddy_enduro.gif)\n",
    "\n",
    "But also a long day in the saddle. For an example, here's the summary of a race on Strava of a pro enduro racer [Jesse Melamed](https://www.strava.com/activities/7260508291) who took 2nd place (by less than half a second to first!). On the clock, his time was 03:00.67 - wheras the total pedaling time was over three and a half hours! \n",
    "\n",
    "![](img/example_ews.png)\n",
    "\n",
    "Enduro racing at the world stage happens in the Enduro World Series, where the best of the best earn points by winning stages and races. At the end of the season a victor is crowned based on the number of points earned. We're going to take a look at the results in these races and look for trends that identify the types of performances that can crown a winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='strava-embed-placeholder' data-embed-type='activity' data-embed-id='7260508291'></div><script src='https://strava-embeds.com/embed.js'></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the data - web scraping\n",
    "The following cells of this notebook download the results from the EWS from 2018 to current. These dates are chosen, as the PDF files in this range are consistent between events! Like any good data science project, data wrangling takes 80% of the time...\n",
    "\n",
    "First, we begin by downloading results and scraping the files from https://www.enduroworldseries.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import typing_extensions\n",
    "import re\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfWriter, PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to make web scraping more pretty. We're using the `requests` package to to download page information and returning in a cleaned up format using `bs4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(url):\n",
    "\theaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\n",
    "\ttry:\n",
    "\t\treq.raise_for_status()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Downloading failed: {e}')\n",
    "\t\n",
    "\treturn bs4.BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download the pages for each of the race years for standard EWS race results and EMTB race results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emtb_result = \"https://www.enduroworldseries.com/races/6/\" \n",
    "ews_result = \"https://www.enduroworldseries.com/races/1/\"\n",
    "\n",
    "emtb_years = [2020, 2021, 2022]\n",
    "ews_years = [2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "list_of_emtb_soup = [download_page(emtb_result + str(year) + '/') for year in emtb_years]\n",
    "\n",
    "list_of_ews_soup = [download_page(ews_result + str(year) + '/') for year in ews_years]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto downlading the actual url's which contain the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_links(results_page, result_url, year):\n",
    "\tlinks = results_page.find_all('a', href=True)\n",
    "\tlinks = [link.get('href') for link in links]\n",
    "\n",
    "\trace_list_URL = [result_url[:-9]+str(link_text) for link_text in links if 'results' in link_text]\n",
    "\n",
    "\treturn [(url, url.split('/')[-4]) for url in race_list_URL if str(year) in url] # returns the tuple of the URL and the name of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_emtb_results = [get_result_links(list_of_emtb_soup[i], emtb_result, emtb_years[i]) for i in range(len(emtb_years))]\n",
    "links_to_ews_results = [get_result_links(list_of_ews_soup[i], ews_result, ews_years[i]) for i in range(len(ews_years))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(soup):\n",
    "\tHTML_source = soup.find_all('a', href=True)\n",
    "\tsource_links = [(link.get('href'), link) for link in HTML_source]\n",
    "\n",
    "\treturn [(pdf, pdf_link.get_text()) for pdf, pdf_link in source_links if '.pdf' in pdf]\n",
    "\n",
    "def download_pdf(url, folder_path, filename):\n",
    "\treq = requests.get(url)\n",
    "\treq.raise_for_status()\n",
    "\n",
    "\twith open(folder_path + filename, 'wb') as f:\n",
    "\t\tf.write(req.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_downloads(result_links, years, folder_path):\n",
    "\tfor i in range(len(years)):\n",
    "\n",
    "\t\tyear = years[i]\n",
    "\n",
    "\t\tfor race_link, race_name in result_links[i]:\n",
    "\n",
    "\t\t\trace_page = download_page(race_link)\n",
    "\t\t\tresult_pdf = find_pdf_links(race_page)\n",
    "\n",
    "\t\t\tfor pdf_link, pdf_text in result_pdf:\n",
    "\n",
    "\t\t\t\tfilename = str(year)+ '_' + race_name + '_' + pdf_text.replace(' ', '_') + '.pdf'\n",
    "\t\t\t\tdownload_pdf(pdf_link, folder_path, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells download all the pdfs for the desired years based off of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_emtb_results, emtb_years, 'emtb_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdf_downloads(links_to_ews_results, ews_years, 'ews_results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the PDF\n",
    "Now that we have read in the various PDF, we need to pull the data out into a useable format. PDF's are tricky beasts, so we're going to rely on the `PyPDF2` package to take these data in. Unfortunately, these PDF are not set up as tables (otherwise this would be a trivial import using the `camelot` package), so we need to use a bunch of regular expressions to extract the desired data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various regular expressions to extract data from text of results PDF\n",
    "stage_numbers_regex = re.compile(r'Stage \\d')\t\t\t\t\t# recognizes stage numbers in headers of PDF\n",
    "position_plate_name_regex = re.compile(r'\\d+\\s\\d+\\s[^a-z]+[a-z]+\\s')\t\t# finds rider position and name from individuals\n",
    "dnf_dns_plate_name_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+\\s[^a-z]+[a-z]+\\s')\t# finds DNF/DNS rider information \n",
    "stage_position_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d \\d+') \t\t\t# recognizes the a stage with its position\n",
    "stage_time_regex = re.compile(r'(\\d:\\d\\d:\\d\\d\\.\\d\\d)') \t\t\t\t# recognizes each stage time (assumes all stages  <10 hours)\n",
    "gap_regex = re.compile(r'\\+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t\t\t# determines gap from overall leader \n",
    "penalty_regex = re.compile(r'\\d:\\d\\d:\\d\\d\\.\\d\\d\\s+\\d:\\d\\d:\\d\\d\\.\\d\\d') \t\t# penalty values occur before overall stage results\n",
    "rider_id_regex = re.compile(r'\\w{3}\\.[\\d\\w\\s]+\\.[\\d\\w]+') \t\t\t# gets rider ID from results\n",
    "lastname_regex = re.compile(r'\\s[^a-z0-9]+')\t\t\t\t\t# recognizes lastname - located between plate and firstname, no lowercase or numbers\n",
    "firstname_regex = re.compile(r'([A-Z][a-z]+\\s)+')\t\t\t\t# recognizes firstname - first capital letter then lowercase TODO make sure this matches correctly \n",
    "position_plate_regex = re.compile(r'\\d+\\s\\d+')\t\t\t\t\t# recognizes the position and plate\n",
    "dnf_dns_plate_regex = re.compile(r'(DNF|DNS|DSQ)\\s\\d+')\t\t\t\t# recognizes DNF/DNS/DSQ along with plate\n",
    "\n",
    "#TODO need better regex for firstname/lastname - see G.T. CLYNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ugly, but this is the function which reads the results PDF files and converts to csv. We convert to csv for easy storage and loading into Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ews_pdf_to_csv(pdf_location, csv_location='csv_output/'):\n",
    "\n",
    "\t# read in PDF and convert each page to list of strings\n",
    "\treader = PdfReader(pdf_location)\n",
    "\tpages = [page.extract_text().split('\\n') for page in reader.pages] # newline separates lines on all pages\n",
    "\n",
    "\tpdf_header = pages[0][:5]\n",
    "\tcolumns = pdf_header[0] + pdf_header[1] # first two lines are the column names for the file\n",
    "\n",
    "\t# race information\n",
    "\tnum_stages = len(stage_numbers_regex.findall(columns)) # store the total number of stages based upon header\n",
    "\trace_date = pdf_header[3]\n",
    "\trace_location = pdf_header[2]\n",
    "\trace_type = 'standard'\n",
    "\n",
    "\trace_info = [race_date, race_location, race_type]\n",
    "\n",
    "\theader_race_info = ['date', 'race_location', 'race_type']\n",
    "\theader_rider_info = ['rider_category','rider_plate', 'rider_lastname', 'rider_firstname', 'rider_id', 'rider_final_position',\n",
    "\t\t'rider_penalties' , 'rider_final_time' , 'gap_from_first']\n",
    "\theader_rider_stage_results = ['stage_'+str(i)+'_time' for i in range(1,num_stages+1)] + ['stage_'+str(i)+'_pos' for i in range(1,num_stages+1)]\n",
    "\t# df_list = [['rider_num', 'rider_name', 'rider_id', 'rider_final_position' + 'rider_final_time'] + ['stage_'+str(i)+'time']]\n",
    "\n",
    "\trace_header_info = [header_race_info + header_rider_info + header_rider_stage_results]\n",
    "\n",
    "\tall_results = race_header_info\n",
    "\n",
    "\n",
    "\tfor page in pages:\n",
    "\n",
    "\t\tis_results_page = pdf_header[0] == page[0] # checks if the first line of the page matches the header\n",
    "\t\t\n",
    "\t\tif is_results_page:\n",
    "\t\t\ti = 5 # start after header\n",
    "\t\t\trider_catagory = ''\n",
    "\n",
    "\t\t\t# iterate over all lines except final (which contains metadata)\n",
    "\t\t\twhile i < len(page) - 1:\n",
    "\n",
    "\t\t\t\tppnr = position_plate_name_regex.search(page[i])\n",
    "\t\t\t\tddr = dnf_dns_plate_name_regex.search(page[i])\n",
    "\n",
    "\t\t\t\tif ppnr or ddr: # check if line contains rider information \n",
    "\t\t\t\t\tresult = copy.deepcopy(race_info)\n",
    "\n",
    "\t\t\t\t\tline1 = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tline2 = page[i]\n",
    "\n",
    "\t\t\t\t\tfix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "\t\t\t\t\tfix = fix.replace('+ ', '+') # removes space before gap time\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tinfo = ppnr.group()\n",
    "\t\t\t\t\t\tppr = position_plate_regex.search(info)\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tinfo = ddr.group()\n",
    "\t\t\t\t\t\tppr = dnf_dns_plate_regex.search(info)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tposition, plate = ppr.group().split(' ')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tlastname = lastname_regex.search(info).group()\n",
    "\t\t\t\t\tlastname = lastname[1:-2]\n",
    "\t\t\t\t\tfirstname = firstname_regex.search(info).group()\n",
    "\n",
    "\t\t\t\t\tspr = stage_position_regex.findall(fix)\n",
    "\t\t\t\t\tspr = [s.split(' ') for s in spr]\t\n",
    "\n",
    "\t\t\t\t\trir = rider_id_regex.search(fix)\n",
    "\t\t\t\t\tpr = penalty_regex.search(line1)\n",
    "\t\t\t\t\tgr = gap_regex.search(fix)\n",
    "\t\t\t\t\tst = stage_time_regex.findall(line1)\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\trider_num = None\n",
    "\t\t\t\t\tif rir:\n",
    "\t\t\t\t\t\trider_num = rir.group()\n",
    "\n",
    "\n",
    "\t\t\t\t\tpenalty_time = None\n",
    "\t\t\t\t\tif pr:\n",
    "\t\t\t\t\t\tpenalty_time = pr.group().split(' ')[0]\n",
    "\n",
    "\t\t\t\t\tresult += [rider_category, plate, lastname, firstname, rider_num, position, penalty_time]\n",
    "\n",
    "\t\t\t\t\tif ppnr:\n",
    "\t\t\t\t\t\tif gr:\n",
    "\t\t\t\t\t\t\tfinal_time = st[-2]\n",
    "\t\t\t\t\t\t\tgap \t   = st[-1]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tfinal_time = st[-1]\n",
    "\t\t\t\t\t\t\tgap = '0:00:00.00'\n",
    "\n",
    "\t\t\t\t\t\tresult += [final_time, gap]\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr]\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult += [None, None] # no gap or final time for DNF/DNS/DSQ\n",
    "\t\t\t\t\t\tstage_diff = num_stages - len(spr) # calculate how many stages were not completed\n",
    "\n",
    "\t\t\t\t\t\tresult += [stage_time for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\t\t\t\t\t\tresult += [stage_pos for stage_time, stage_pos in spr] + [None for _ in range(stage_diff)]\n",
    "\n",
    "\t\t\t\t\tall_results.append(result)\n",
    "\t\t\t\t\ti += 1\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telse:\t# otherwise, this is category information for the following riders\n",
    "\t\t\t\t\trider_category = page[i]\n",
    "\t\t\t\t\ti += 1\n",
    "\t\n",
    "\tpdf_filename = os.path.split(pdf_location)[1][:-4]\n",
    "\n",
    "\twith open(csv_location + pdf_filename + '.csv', 'w', newline='') as cw:\n",
    "\t\twriter = csv.writer(cw)\n",
    "\t\tfor row in all_results:\n",
    "\t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ews_pdf_to_csv(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('raw_pdf', 'test3.pdf')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(\"raw_pdf/test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv_output/test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[-1][0] == pdf_header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results[0]) == len(all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March 24 - 25, 2018',\n",
       " 'Lo Barnechea, Chile',\n",
       " 'standard',\n",
       " 'MEN',\n",
       " '1',\n",
       " 'HILL',\n",
       " 'Sam',\n",
       " 'AUS.1985.21775',\n",
       " '1',\n",
       " None,\n",
       " '0:55:02.18',\n",
       " '0:00:00.00',\n",
       " '0:05:12.25',\n",
       " '0:18:07.96',\n",
       " '0:05:26.70',\n",
       " '0:09:55.25',\n",
       " '0:04:07.43',\n",
       " '0:12:12.59',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2 = page2text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = '4 13 HILL Sam 0:03:28.58 22 0:05:44.14 28 0:04:55.69 190:06:44.10 1 0:37:40.33 +0:00:08.61'\n",
    "line2 = '  Chain Reaction Cycles Mavic AUS.HILS.1985 0:07:19.73 7 0:04:17.81 20:05:10.28 1'\n",
    "\n",
    "line1 = '147 122 DA SILVA Goncalo 0:04:11.38 125 0:09:38.38 157 0:07:50.37 154 0:11:31.81 153 0:01:00.00 1:10:34.72 +0:33:03.00'\n",
    "line2 = ' POR.DA G.1987 0:19:28.41 147 0:08:13.09 148 0:08:41.28 148'\n",
    "\n",
    "fix = stage_time_regex.sub(r' \\1', line1+line2) # adds space before each stage time - used to fix issue with formatting of underlines\n",
    "fix = fix.replace('+ ', '+') # removes space before gap time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'147 122 DA SILVA Goncalo  0:04:11.38 125  0:09:38.38 157  0:07:50.37 154  0:11:31.81 153  0:01:00.00  1:10:34.72 +0:33:03.00 POR.DA G.1987  0:19:28.41 147  0:08:13.09 148  0:08:41.28 148'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnr = position_plate_name_regex.search(fix)\n",
    "spr = stage_position_regex.findall(fix)\n",
    "rir = rider_id_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = penalty_regex.search(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0:01:00.00  1:10:34.72'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'147 122 DA SILVA Goncalo '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppnr.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = [s.split(' ') for s in spr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0:04:11.38', '125'],\n",
       " ['0:09:38.38', '157'],\n",
       " ['0:07:50.37', '154'],\n",
       " ['0:11:31.81', '153'],\n",
       " ['0:19:28.41', '147'],\n",
       " ['0:08:13.09', '148'],\n",
       " ['0:08:41.28', '148']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POR.DA G.1987'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rir.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95fc6b46201b03e388ee93255aacead15bb4c5a805a1325bd67fb6d36cada86c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
